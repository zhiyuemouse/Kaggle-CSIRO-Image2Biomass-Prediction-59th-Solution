{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:22.667921Z",
     "iopub.status.busy": "2025-12-24T13:44:22.667581Z",
     "iopub.status.idle": "2025-12-24T13:44:34.947624Z",
     "shell.execute_reply": "2025-12-24T13:44:34.946966Z",
     "shell.execute_reply.started": "2025-12-24T13:44:22.667870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings # 避免一些可以忽略的报错\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler # 学习率调度器\n",
    "from torch.optim.lr_scheduler import _LRScheduler, CosineAnnealingLR\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>Sampling_Date</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Pre_GSHH_NDVI</th>\n",
       "      <th>Height_Ave_cm</th>\n",
       "      <th>Dry_Clover_g</th>\n",
       "      <th>Dry_Dead_g</th>\n",
       "      <th>Dry_Green_g</th>\n",
       "      <th>Dry_Total_g</th>\n",
       "      <th>...</th>\n",
       "      <th>emb1152</th>\n",
       "      <th>bin_total</th>\n",
       "      <th>clover_frac</th>\n",
       "      <th>bin_clover</th>\n",
       "      <th>state_key</th>\n",
       "      <th>key_L1</th>\n",
       "      <th>key_L2</th>\n",
       "      <th>key_L3</th>\n",
       "      <th>final_stratify</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1011485656...</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass_Clover</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>31.9984</td>\n",
       "      <td>16.2751</td>\n",
       "      <td>48.2735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027203</td>\n",
       "      <td>M</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Lo</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_M_Lo</td>\n",
       "      <td>Tas_M</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_M_Lo</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1012260530...</td>\n",
       "      <td>2015/4/1</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Lucerne</td>\n",
       "      <td>0.55</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.6000</td>\n",
       "      <td>7.6000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001485</td>\n",
       "      <td>L</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Lo</td>\n",
       "      <td>NSW</td>\n",
       "      <td>NSW_L_Lo</td>\n",
       "      <td>NSW_L</td>\n",
       "      <td>NSW</td>\n",
       "      <td>NSW_L_Lo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1025234388...</td>\n",
       "      <td>2015/9/1</td>\n",
       "      <td>WA</td>\n",
       "      <td>SubcloverDalkeith</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>6.0500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030815</td>\n",
       "      <td>L</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Hi</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA_L_Hi</td>\n",
       "      <td>WA_L</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA_L_Hi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1028611175...</td>\n",
       "      <td>2015/5/18</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass</td>\n",
       "      <td>0.66</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>30.9703</td>\n",
       "      <td>24.2376</td>\n",
       "      <td>55.2079</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020800</td>\n",
       "      <td>H</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Lo</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_H_Lo</td>\n",
       "      <td>Tas_H</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_H_Lo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID1035947949...</td>\n",
       "      <td>2015/9/11</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Ryegrass</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>0.4343</td>\n",
       "      <td>23.2239</td>\n",
       "      <td>10.5261</td>\n",
       "      <td>34.1844</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031969</td>\n",
       "      <td>M</td>\n",
       "      <td>0.039624</td>\n",
       "      <td>Lo</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_M_Lo</td>\n",
       "      <td>Tas_M</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_M_Lo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID975115267.jpg</td>\n",
       "      <td>2015/7/8</td>\n",
       "      <td>WA</td>\n",
       "      <td>Clover</td>\n",
       "      <td>0.73</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>40.0300</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>40.8300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033440</td>\n",
       "      <td>M</td>\n",
       "      <td>0.980407</td>\n",
       "      <td>Hi</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA_M_Hi</td>\n",
       "      <td>WA_M</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA_M_Hi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID978026131.jpg</td>\n",
       "      <td>2015/9/4</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Clover</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.1667</td>\n",
       "      <td>24.6445</td>\n",
       "      <td>4.1948</td>\n",
       "      <td>12.0601</td>\n",
       "      <td>40.8994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022025</td>\n",
       "      <td>M</td>\n",
       "      <td>0.671428</td>\n",
       "      <td>Hi</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_M_Hi</td>\n",
       "      <td>Tas_M</td>\n",
       "      <td>Tas</td>\n",
       "      <td>Tas_M_Hi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID980538882.jpg</td>\n",
       "      <td>2015/2/24</td>\n",
       "      <td>NSW</td>\n",
       "      <td>Phalaris</td>\n",
       "      <td>0.69</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.1457</td>\n",
       "      <td>91.6543</td>\n",
       "      <td>92.8000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010250</td>\n",
       "      <td>H</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Lo</td>\n",
       "      <td>NSW</td>\n",
       "      <td>NSW_H_Lo</td>\n",
       "      <td>NSW_H</td>\n",
       "      <td>NSW</td>\n",
       "      <td>NSW_H_Lo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID980878870.jpg</td>\n",
       "      <td>2015/7/8</td>\n",
       "      <td>WA</td>\n",
       "      <td>Clover</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>32.3575</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.0325</td>\n",
       "      <td>34.3900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028169</td>\n",
       "      <td>M</td>\n",
       "      <td>0.940898</td>\n",
       "      <td>Hi</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA_M_Hi</td>\n",
       "      <td>WA_M</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA_M_Hi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>/kaggle/input/csiro-biomass/train/ID983582017.jpg</td>\n",
       "      <td>2015/9/1</td>\n",
       "      <td>WA</td>\n",
       "      <td>Ryegrass</td>\n",
       "      <td>0.64</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>40.9400</td>\n",
       "      <td>40.9400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>M</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Lo</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA_M_Lo</td>\n",
       "      <td>WA_M</td>\n",
       "      <td>WA</td>\n",
       "      <td>WA</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 1172 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path Sampling_Date State  \\\n",
       "0    /kaggle/input/csiro-biomass/train/ID1011485656...      2015/9/4   Tas   \n",
       "1    /kaggle/input/csiro-biomass/train/ID1012260530...      2015/4/1   NSW   \n",
       "2    /kaggle/input/csiro-biomass/train/ID1025234388...      2015/9/1    WA   \n",
       "3    /kaggle/input/csiro-biomass/train/ID1028611175...     2015/5/18   Tas   \n",
       "4    /kaggle/input/csiro-biomass/train/ID1035947949...     2015/9/11   Tas   \n",
       "..                                                 ...           ...   ...   \n",
       "352  /kaggle/input/csiro-biomass/train/ID975115267.jpg      2015/7/8    WA   \n",
       "353  /kaggle/input/csiro-biomass/train/ID978026131.jpg      2015/9/4   Tas   \n",
       "354  /kaggle/input/csiro-biomass/train/ID980538882.jpg     2015/2/24   NSW   \n",
       "355  /kaggle/input/csiro-biomass/train/ID980878870.jpg      2015/7/8    WA   \n",
       "356  /kaggle/input/csiro-biomass/train/ID983582017.jpg      2015/9/1    WA   \n",
       "\n",
       "               Species  Pre_GSHH_NDVI  Height_Ave_cm  Dry_Clover_g  \\\n",
       "0      Ryegrass_Clover           0.62         4.6667        0.0000   \n",
       "1              Lucerne           0.55        16.0000        0.0000   \n",
       "2    SubcloverDalkeith           0.38         1.0000        6.0500   \n",
       "3             Ryegrass           0.66         5.0000        0.0000   \n",
       "4             Ryegrass           0.54         3.5000        0.4343   \n",
       "..                 ...            ...            ...           ...   \n",
       "352             Clover           0.73         3.0000       40.0300   \n",
       "353             Clover           0.83         3.1667       24.6445   \n",
       "354           Phalaris           0.69        29.0000        0.0000   \n",
       "355             Clover           0.74         2.0000       32.3575   \n",
       "356           Ryegrass           0.64         9.0000        0.0000   \n",
       "\n",
       "     Dry_Dead_g  Dry_Green_g  Dry_Total_g  ...   emb1152  bin_total  \\\n",
       "0       31.9984      16.2751      48.2735  ... -0.027203          M   \n",
       "1        0.0000       7.6000       7.6000  ... -0.001485          L   \n",
       "2        0.0000       0.0000       6.0500  ... -0.030815          L   \n",
       "3       30.9703      24.2376      55.2079  ... -0.020800          H   \n",
       "4       23.2239      10.5261      34.1844  ... -0.031969          M   \n",
       "..          ...          ...          ...  ...       ...        ...   \n",
       "352      0.0000       0.8000      40.8300  ... -0.033440          M   \n",
       "353      4.1948      12.0601      40.8994  ... -0.022025          M   \n",
       "354      1.1457      91.6543      92.8000  ... -0.010250          H   \n",
       "355      0.0000       2.0325      34.3900  ... -0.028169          M   \n",
       "356      0.0000      40.9400      40.9400  ... -0.000232          M   \n",
       "\n",
       "     clover_frac  bin_clover  state_key    key_L1  key_L2  key_L3  \\\n",
       "0       0.000000          Lo        Tas  Tas_M_Lo   Tas_M     Tas   \n",
       "1       0.000000          Lo        NSW  NSW_L_Lo   NSW_L     NSW   \n",
       "2       1.000000          Hi         WA   WA_L_Hi    WA_L      WA   \n",
       "3       0.000000          Lo        Tas  Tas_H_Lo   Tas_H     Tas   \n",
       "4       0.039624          Lo        Tas  Tas_M_Lo   Tas_M     Tas   \n",
       "..           ...         ...        ...       ...     ...     ...   \n",
       "352     0.980407          Hi         WA   WA_M_Hi    WA_M      WA   \n",
       "353     0.671428          Hi        Tas  Tas_M_Hi   Tas_M     Tas   \n",
       "354     0.000000          Lo        NSW  NSW_H_Lo   NSW_H     NSW   \n",
       "355     0.940898          Hi         WA   WA_M_Hi    WA_M      WA   \n",
       "356     0.000000          Lo         WA   WA_M_Lo    WA_M      WA   \n",
       "\n",
       "     final_stratify  fold  \n",
       "0          Tas_M_Lo     4  \n",
       "1          NSW_L_Lo     0  \n",
       "2           WA_L_Hi     3  \n",
       "3          Tas_H_Lo     2  \n",
       "4          Tas_M_Lo     2  \n",
       "..              ...   ...  \n",
       "352         WA_M_Hi     2  \n",
       "353        Tas_M_Hi     2  \n",
       "354        NSW_H_Lo     1  \n",
       "355         WA_M_Hi     0  \n",
       "356              WA     3  \n",
       "\n",
       "[357 rows x 1172 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"CSIRO/csiro_data_split.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:34.949249Z",
     "iopub.status.busy": "2025-12-24T13:44:34.948979Z",
     "iopub.status.idle": "2025-12-24T13:44:35.011668Z",
     "shell.execute_reply": "2025-12-24T13:44:35.010763Z",
     "shell.execute_reply.started": "2025-12-24T13:44:34.949227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    is_debug = False\n",
    "    seed = 308\n",
    "    n_folds = 5\n",
    "    n_workers = os.cpu_count() // 2\n",
    "\n",
    "    # train_csv = \"/kaggle/input/csiro-biomass/train.csv\" # 官方 csv\n",
    "    train_csv = \"CSIRO/CSIRO_my_5fold_train_csv.csv\" # 分过 5 折的 csv\n",
    "    train_img_path = \"CSIRO/train\" # (1000, 2000)\n",
    "    pretrain_ckpt_path = \"CSIRO/ckpt\"\n",
    "\n",
    "    train_batch_size = 4\n",
    "    valid_batch_size = 16\n",
    "    now_cv = -np.inf\n",
    "\n",
    "    epochs = 10\n",
    "    start_lr_backbone = 1e-5\n",
    "    start_lr_head = 1e-3\n",
    "    min_lr_backbone = 1e-8\n",
    "    min_lr_head = 1e-6\n",
    "    scheduler = 'CosineAnnealingWithWarmupLR'\n",
    "    n_accumulate = 1.0\n",
    "    ckpt_save_path = None\n",
    "    T_max = (357 // (n_folds * train_batch_size) + 1) * (n_folds - 1) * epochs\n",
    "\n",
    "    model_name = \"convnextv2_tiny.fcmae_ft_in22k_in1k\"\n",
    "    if \"dino\" in model_name:\n",
    "        img_size = [518, 1036]\n",
    "    else:\n",
    "        img_size = [512, 1024]\n",
    "    # img_size = [384, 768]\n",
    "    \"\"\"\n",
    "    tf_efficientnet_b0.ns_jft_in1k\n",
    "    edgenext_base.in21k_ft_in1k\n",
    "    vit_base_patch14_dinov2.lvd142m\n",
    "    convnextv2_tiny.fcmae_ft_in22k_in1k\n",
    "\n",
    "    convnext_large_mlp.laion2b_ft_augreg_inat21\n",
    "    resnet50.a1_in1k_ft_inat21\n",
    "    efficientnet_b5.in1k_ft_inat21\n",
    "    seresnext50_32x4d.racm_in1k_ft_inat21\n",
    "    vit_base_patch14_dinov2.lvd142m\n",
    "    vit_small_patch14_dinov2.lvd142m\n",
    "    convnextv2_base.fcmae_ft_in22k_in1k\n",
    "    \"\"\"\n",
    "    is_pretrained = True\n",
    "    head_out = 5\n",
    "    DataParallel = False\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.013071Z",
     "iopub.status.busy": "2025-12-24T13:44:35.012801Z",
     "iopub.status.idle": "2025-12-24T13:44:35.037106Z",
     "shell.execute_reply": "2025-12-24T13:44:35.036495Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.013048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=308):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.039537Z",
     "iopub.status.busy": "2025-12-24T13:44:35.038743Z",
     "iopub.status.idle": "2025-12-24T13:44:35.043211Z",
     "shell.execute_reply": "2025-12-24T13:44:35.042405Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.039504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ## KFold\n",
    "\n",
    "# # 读取并生成了 'image_group_id'\n",
    "# df = pd.read_csv(CONFIG.train_csv)\n",
    "# df['image_group_id'] = df['sample_id'].apply(lambda x: x.split('__')[0])\n",
    "\n",
    "# # --- 步骤 1: 统计每个物种有多少个唯一的 Group (图片数) ---\n",
    "# species_counts = df.groupby('Species')['image_group_id'].nunique()\n",
    "# print(\"各物种图片数量统计：\")\n",
    "# print(species_counts)\n",
    "\n",
    "# # --- 步骤 2: 定义稀有阈值 ---\n",
    "# # 如果你想做 5 折，理论上至少需要 5 张图才能保证每折一张。\n",
    "# # 为了保险，我们可以把少于 10 张图的都归为 'Other'\n",
    "# threshold = 10 \n",
    "# rare_species = species_counts[species_counts < threshold].index.tolist()\n",
    "\n",
    "# print(f\"\\n将被归类为 'Other' 的稀有物种: {rare_species}\")\n",
    "\n",
    "# # --- 步骤 3: 创建用于分层的临时列 ---\n",
    "# # 如果是稀有物种，标记为 'Other'，否则保持原名\n",
    "# df['stratify_col'] = df['Species'].apply(lambda x: 'Other' if x in rare_species else x)\n",
    "\n",
    "# # --- 步骤 4: 重新进行 StratifiedGroupKFold ---\n",
    "# sgkf = StratifiedGroupKFold(n_splits=CONFIG.n_folds)\n",
    "# df['fold'] = -1\n",
    "\n",
    "# for fold_id, (train_idx, val_idx) in enumerate(sgkf.split(X=df, y=df['stratify_col'], groups=df['image_group_id'])):\n",
    "#     df.loc[val_idx, 'fold'] = fold_id\n",
    "\n",
    "# # --- 步骤 5: 再次检查分布 ---\n",
    "# print(\"\\n优化后的分布检查 (注意 Other 类的分布):\")\n",
    "# print(df.groupby(['fold', 'stratify_col']).size().unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.044447Z",
     "iopub.status.busy": "2025-12-24T13:44:35.044099Z",
     "iopub.status.idle": "2025-12-24T13:44:35.111431Z",
     "shell.execute_reply": "2025-12-24T13:44:35.110810Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.044417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_all = pd.read_csv(CONFIG.train_csv)\n",
    "train_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.112451Z",
     "iopub.status.busy": "2025-12-24T13:44:35.112182Z",
     "iopub.status.idle": "2025-12-24T13:44:35.249084Z",
     "shell.execute_reply": "2025-12-24T13:44:35.248477Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.112429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "id_and_fold = {}\n",
    "for i in range(len(train_all)):\n",
    "    row = train_all.iloc[i, :]\n",
    "    _id = row.sample_id.split(\"_\")[0]\n",
    "    _fold = row.fold.item()\n",
    "    if _id not in id_and_fold.keys():\n",
    "        id_and_fold[_id] = _fold\n",
    "\n",
    "train = pd.DataFrame(list(id_and_fold.items()), columns=['sample_id', 'fold'])\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.250294Z",
     "iopub.status.busy": "2025-12-24T13:44:35.249944Z",
     "iopub.status.idle": "2025-12-24T13:44:35.256974Z",
     "shell.execute_reply": "2025-12-24T13:44:35.256327Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.250271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. 获取配置对象\n",
    "cfg = timm.get_pretrained_cfg(CONFIG.model_name)\n",
    "\n",
    "# 2. 【核心修复】先转成字典 (.to_dict()) 再传入\n",
    "# 这样 resolve_data_config 就能正常使用 .get() 方法了\n",
    "cfg_dict = cfg.to_dict()\n",
    "data_config = timm.data.resolve_data_config(pretrained_cfg=cfg_dict)\n",
    "\n",
    "# 3. 提取结果\n",
    "_mean = data_config['mean']\n",
    "_std = data_config['std']\n",
    "\n",
    "print(f\"模型: {CONFIG.model_name}\")\n",
    "print(f\"自动获取 Mean: {_mean}\")\n",
    "print(f\"自动获取 Std:  {_std}\")\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\n",
    "def transform(img):\n",
    "    composition = A.Compose([\n",
    "        A.Resize(CONFIG.img_size[0], CONFIG.img_size[1]),\n",
    "        A.Normalize(\n",
    "            mean=_mean,\n",
    "            std=_std\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    return composition(image=img)[\"image\"]\n",
    "\n",
    "def transform_train(img):\n",
    "    if random.random() < 0.5:\n",
    "        # =================================================\n",
    "        # Path A: 50% 概率 - 原始 Transform (无增强)\n",
    "        # =================================================\n",
    "        composition = A.Compose([\n",
    "            A.Resize(CONFIG.img_size[0], CONFIG.img_size[0]), # 这里resize到 (512, 512)\n",
    "            A.Normalize(mean=_mean, std=_std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        composition = A.Compose([\n",
    "            A.Resize(CONFIG.img_size[0], CONFIG.img_size[0]),\n",
    "            # 几何增强 (适合俯拍的草地/植物)\n",
    "            A.HorizontalFlip(p=0.5),      # 水平翻转\n",
    "            A.VerticalFlip(p=0.5),        # 垂直翻转\n",
    "            A.RandomRotate90(p=0.5),      # 90度旋转 (草地没有方向性，这个很好用)\n",
    "            \n",
    "            # 像素/颜色增强 (模拟不同天气、光照、模糊)\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(p=1.0), # 亮度对比度\n",
    "                A.HueSaturationValue(p=1.0),       # 色调饱和度\n",
    "                A.GaussNoise(p=1.0),               # 高斯噪点\n",
    "            ], p=0.5), # 这组增强有 50% 概率触发\n",
    "\n",
    "            A.Normalize(mean=_mean, std=_std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    \n",
    "    return composition(image=img)[\"image\"]\n",
    "\n",
    "def transform_valid(img):\n",
    "    composition = A.Compose([\n",
    "        A.Resize(CONFIG.img_size[0], CONFIG.img_size[1]), # 这里resize到 (512, 1024)\n",
    "        A.Normalize(\n",
    "            mean=_mean,\n",
    "            std=_std\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    return composition(image=img)[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.258204Z",
     "iopub.status.busy": "2025-12-24T13:44:35.257954Z",
     "iopub.status.idle": "2025-12-24T13:44:35.274128Z",
     "shell.execute_reply": "2025-12-24T13:44:35.273372Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.258184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CSIRODataset(Dataset):\n",
    "    def __init__(self, df, original_train=train_all, transform=transform):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.original_train = original_train\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx, :]\n",
    "        img_id = row.sample_id\n",
    "\n",
    "        img_path = os.path.join(CONFIG.train_img_path, img_id + \".jpg\")\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(img)\n",
    "        if self.transform != None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        target_id = [\"__Dry_Clover_g\", \"__Dry_Dead_g\", \"__Dry_Green_g\", \"__Dry_Total_g\", \"__GDM_g\"]\n",
    "        label = []\n",
    "        for _id in target_id:\n",
    "            tmp_row = self.original_train[self.original_train[\"sample_id\"] == f\"{img_id}{_id}\"][\"target\"].item()\n",
    "            label.append(tmp_row)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.275275Z",
     "iopub.status.busy": "2025-12-24T13:44:35.275004Z",
     "iopub.status.idle": "2025-12-24T13:44:35.288480Z",
     "shell.execute_reply": "2025-12-24T13:44:35.287847Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.275245Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_loaders(df, fold=0):\n",
    "    df_train = df[df[\"fold\"] != fold]\n",
    "    df_valid = df[df[\"fold\"] == fold]\n",
    "    \n",
    "    train_datasets = CSIRODataset(df=df_train, transform=transform_train)\n",
    "    valid_datasets = CSIRODataset(df=df_valid, transform=transform_valid)\n",
    "    \n",
    "    train_loader = DataLoader(train_datasets, batch_size=CONFIG.train_batch_size, num_workers=CONFIG.n_workers, shuffle=True, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_datasets, batch_size=CONFIG.valid_batch_size, num_workers=CONFIG.n_workers, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:35.291358Z",
     "iopub.status.busy": "2025-12-24T13:44:35.290935Z",
     "iopub.status.idle": "2025-12-24T13:44:39.557087Z",
     "shell.execute_reply": "2025-12-24T13:44:39.556362Z",
     "shell.execute_reply.started": "2025-12-24T13:44:35.291329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 以下代码可检查Dataset，DataLoader是否实现基本功能\n",
    "train_loader, valid_loader = prepare_loaders(train, 0)\n",
    "x_train, y_train = next(iter(train_loader))\n",
    "x_valid, y_valid = next(iter(valid_loader))\n",
    "print(f\"X_train shape : {x_train.shape}\") # (batch_size, channels, H, W)\n",
    "print(f\"y_train shape : {y_train.shape}\")\n",
    "print(f\"x_valid shape : {x_valid.shape}\")\n",
    "print(f\"y_valid shape : {y_valid.shape}\")\n",
    "\n",
    "# 删除变量，回收垃圾\n",
    "del train_loader, valid_loader, x_train, y_train, x_valid, y_valid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:39.558464Z",
     "iopub.status.busy": "2025-12-24T13:44:39.558213Z",
     "iopub.status.idle": "2025-12-24T13:44:39.569335Z",
     "shell.execute_reply": "2025-12-24T13:44:39.568696Z",
     "shell.execute_reply.started": "2025-12-24T13:44:39.558442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Calculate_Weighted_R2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    计算 Kaggle CSIRO Image2Biomass 比赛的加权 R2 分数。\n",
    "    \n",
    "    参数:\n",
    "    y_true: 真实值，形状为 [n_samples, 5]\n",
    "    y_pred: 预测值，形状为 [n_samples, 5]\n",
    "    \n",
    "    列顺序假设:\n",
    "    0: Dry_Clover_g (w=0.1)\n",
    "    1: Dry_Dead_g   (w=0.1)\n",
    "    2: Dry_Green_g  (w=0.1)\n",
    "    3: Dry_Total_g  (w=0.5)\n",
    "    4: GDM_g        (w=0.2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 定义权重向量\n",
    "    weights = np.array([0.1, 0.1, 0.1, 0.5, 0.2])\n",
    "    \n",
    "    # 2. 确保输入是 numpy 数组\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # 3. 计算全局加权均值 (Global Weighted Mean)\n",
    "    # 这里的 sum(weights) = 1.0，所以分母实际上就是 样本数 * 1.0\n",
    "    # 我们利用广播机制将权重应用到每一行\n",
    "    weighted_sum = np.sum(y_true * weights) \n",
    "    total_weight = np.sum(weights) * y_true.shape[0] # weights总和 * 样本数\n",
    "    y_bar_w = weighted_sum / total_weight\n",
    "    \n",
    "    # 4. 计算残差平方和 (SS_res)\n",
    "    # 公式: sum( w_j * (y_j - y_hat_j)^2 )\n",
    "    ss_res = np.sum(weights * (y_true - y_pred)**2)\n",
    "    \n",
    "    # 5. 计算总离差平方和 (SS_tot)\n",
    "    # 公式: sum( w_j * (y_j - y_bar_w)^2 )\n",
    "    # 注意这里减去的是全局加权均值 y_bar_w\n",
    "    ss_tot = np.sum(weights * (y_true - y_bar_w)**2)\n",
    "    \n",
    "    # 6. 计算 R2\n",
    "    # 避免分母为0的极个别情况\n",
    "    if ss_tot == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return r2\n",
    "\n",
    "# --- 测试用例 ---\n",
    "# 模拟数据\n",
    "dummy_true = np.array([\n",
    "    [5, 16, 36, 54, 42],\n",
    "    [3, 8, 10, 18, 13]\n",
    "])\n",
    "# 假设预测非常接近\n",
    "dummy_pred = dummy_true + 0.5 \n",
    "\n",
    "score = Calculate_Weighted_R2(dummy_true, dummy_pred)\n",
    "print(f\"验证集得分: {score:.5f}\")\n",
    "\n",
    "# 使用示例\n",
    "# 模拟验证集数据\n",
    "n_valid_samples = 16\n",
    "# 随机生成验证集真实值和预测值\n",
    "y_valid_true = np.random.rand(n_valid_samples, 5)\n",
    "y_valid_pred = np.random.rand(n_valid_samples, 5)\n",
    "# 计算分数\n",
    "score = Calculate_Weighted_R2(y_valid_true, y_valid_pred).item()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:39.570571Z",
     "iopub.status.busy": "2025-12-24T13:44:39.570330Z",
     "iopub.status.idle": "2025-12-24T13:44:39.593868Z",
     "shell.execute_reply": "2025-12-24T13:44:39.593145Z",
     "shell.execute_reply.started": "2025-12-24T13:44:39.570549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def Calculate_Weighted_R2(y_true, y_pred): # K2 感觉不太行\n",
    "#     \"\"\"\n",
    "#     计算Kaggle比赛的全局加权R²分数\n",
    "    \n",
    "#     参数:\n",
    "#     -----------\n",
    "#     y_true : array-like of shape (n_samples, n_features)\n",
    "#         真实值，特征顺序必须是：[\"__Dry_Clover_g\", \"__Dry_Dead_g\", \"__Dry_Green_g\", \"__Dry_Total_g\", \"__GDM_g\"]\n",
    "#     y_pred : array-like of shape (n_samples, n_features)\n",
    "#         预测值，形状与y_true相同\n",
    "    \n",
    "#     返回:\n",
    "#     -----------\n",
    "#     float\n",
    "#         加权R²分数\n",
    "#     \"\"\"\n",
    "#     # 转换为numpy数组（处理torch.Tensor和numpy数组两种情况）\n",
    "#     if isinstance(y_true, torch.Tensor):\n",
    "#         y_true = y_true.cpu().numpy()\n",
    "#     if isinstance(y_pred, torch.Tensor):\n",
    "#         y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "#     y_true = np.asarray(y_true)\n",
    "#     y_pred = np.asarray(y_pred)\n",
    "    \n",
    "#     # 验证输入形状\n",
    "#     assert y_true.shape == y_pred.shape, \"y_true和y_pred的形状必须相同\"\n",
    "#     assert y_true.ndim == 2, \"输入必须是2维数组，形状为[样本数量, 特征数量]\"\n",
    "#     assert y_true.shape[1] == 5, f\"特征数量必须是5，当前是{y_true.shape[1]}\"\n",
    "    \n",
    "#     # 特征权重映射（根据官方评估方案）\n",
    "#     # 注意：权重是根据目标类型（特征）而不是样本分配的\n",
    "#     feature_weights = np.array([\n",
    "#         0.1,  # __Dry_Clover_g\n",
    "#         0.1,  # __Dry_Dead_g  \n",
    "#         0.1,  # __Dry_Green_g\n",
    "#         0.5,  # __Dry_Total_g\n",
    "#         0.2   # __GDM_g\n",
    "#     ])\n",
    "    \n",
    "#     # 展平数组（官方要求：所有(image, target)对一起计算）\n",
    "#     y_true_flat = y_true.flatten()  # 形状: (n_samples * n_features,)\n",
    "#     y_pred_flat = y_pred.flatten()  # 形状: (n_samples * n_features,)\n",
    "    \n",
    "#     # 为每个展平后的值创建对应的权重\n",
    "#     # 每个样本的5个特征循环使用feature_weights\n",
    "#     weights_flat = np.tile(feature_weights, y_true.shape[0])\n",
    "    \n",
    "#     # 计算全局加权均值 y_w\n",
    "#     y_w = np.sum(weights_flat * y_true_flat) / np.sum(weights_flat)\n",
    "    \n",
    "#     # 计算残差平方和 SSres\n",
    "#     ss_res = np.sum(weights_flat * (y_true_flat - y_pred_flat) ** 2)\n",
    "    \n",
    "#     # 计算总平方和 SStot\n",
    "#     ss_tot = np.sum(weights_flat * (y_true_flat - y_w) ** 2)\n",
    "    \n",
    "#     # 防止除零错误\n",
    "#     if ss_tot == 0:\n",
    "#         return 1.0 if ss_res == 0 else 0.0\n",
    "    \n",
    "#     # 计算加权R²\n",
    "#     r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "#     return r2\n",
    "\n",
    "# # 使用示例\n",
    "# # 模拟验证集数据\n",
    "# n_valid_samples = 16\n",
    "# # 随机生成验证集真实值和预测值\n",
    "# y_valid_true = np.random.rand(n_valid_samples, 5)\n",
    "# y_valid_pred = np.random.rand(n_valid_samples, 5)\n",
    "# # 计算分数\n",
    "# score = Calculate_Weighted_R2(y_valid_true, y_valid_pred).item()\n",
    "# score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:39.595360Z",
     "iopub.status.busy": "2025-12-24T13:44:39.595109Z",
     "iopub.status.idle": "2025-12-24T13:44:39.614549Z",
     "shell.execute_reply": "2025-12-24T13:44:39.613859Z",
     "shell.execute_reply.started": "2025-12-24T13:44:39.595339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CSIROModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSIROModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name=CONFIG.model_name, \n",
    "                                          pretrained=False)\n",
    "        if CONFIG.is_pretrained:\n",
    "            self.backbone.load_state_dict(torch.load(f\"{CONFIG.pretrain_ckpt_path}/{CONFIG.model_name}.pth\"))\n",
    "\n",
    "        if \"efficientnet\" in CONFIG.model_name:\n",
    "            in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif \"edgenext\" in CONFIG.model_name:\n",
    "            in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        elif \"convnext\" in CONFIG.model_name:\n",
    "            in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "        elif \"dino\" in CONFIG.model_name:\n",
    "            in_features = 1536\n",
    "        else:\n",
    "            raise(\"Error model!\")\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features // 2, CONFIG.head_out),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if \"dino\" in CONFIG.model_name:\n",
    "            mid = CONFIG.img_size[0]\n",
    "            x_left = x[:, :, :, :mid]\n",
    "            x_right = x[:, :, :, mid:]\n",
    "            _tmp1 = self.backbone(x_left)\n",
    "            _tmp2 = self.backbone(x_right)\n",
    "            _tmp = torch.cat([_tmp1, _tmp2], dim=1) # shape: [B, 1536]\n",
    "        else:\n",
    "            _tmp = self.backbone(x)\n",
    "        output = self.head(_tmp)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:39.615982Z",
     "iopub.status.busy": "2025-12-24T13:44:39.615689Z",
     "iopub.status.idle": "2025-12-24T13:44:42.779362Z",
     "shell.execute_reply": "2025-12-24T13:44:42.778642Z",
     "shell.execute_reply.started": "2025-12-24T13:44:39.615952Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = CSIROModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Valid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.780562Z",
     "iopub.status.busy": "2025-12-24T13:44:42.780262Z",
     "iopub.status.idle": "2025-12-24T13:44:42.786729Z",
     "shell.execute_reply": "2025-12-24T13:44:42.786137Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.780532Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# 加权MSE损失\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, feature_weights=None):\n",
    "        super().__init__()\n",
    "        if feature_weights is None:\n",
    "            # 权重\n",
    "            self.register_buffer('feature_weights', \n",
    "                               torch.tensor([0.1, 0.1, 0.1, 0.5, 0.2]))\n",
    "        else:\n",
    "            self.register_buffer('feature_weights', torch.tensor(feature_weights))\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # 确保权重设备和类型正确\n",
    "        weights = self.feature_weights.to(device=y_pred.device, dtype=y_pred.dtype)\n",
    "        \n",
    "        # 1. 计算加权平方误差 (Batch_Size, 5)\n",
    "        loss = weights * (y_pred - y_true) ** 2\n",
    "        \n",
    "        # 2. 关键修改：\n",
    "        # 先在 dim=1 (特征维度) 求和 -> 得到每个样本的加权 Error Sum\n",
    "        # 再在 dim=0 (Batch维度) 求平均 -> 得到 Batch 的平均 Loss\n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "# 实例化损失函数\n",
    "criterion = WeightedMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.788311Z",
     "iopub.status.busy": "2025-12-24T13:44:42.787730Z",
     "iopub.status.idle": "2025-12-24T13:44:42.805578Z",
     "shell.execute_reply": "2025-12-24T13:44:42.804834Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.788278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (images, labels) in bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        if CONFIG.DataParallel:\n",
    "            images = images.cuda().float()\n",
    "            labels = labels.cuda().float()\n",
    "        else:\n",
    "            images = images.to(CONFIG.device, dtype=torch.float)\n",
    "            labels = labels.to(CONFIG.device, dtype=torch.float)\n",
    "            \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) / CONFIG.n_accumulate\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % CONFIG.n_accumulate == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        y_preds.append(outputs.detach().cpu().numpy())\n",
    "        y_trues.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        train_cv = Calculate_Weighted_R2(np.concatenate(y_trues), np.concatenate(y_preds))\n",
    "\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch,\n",
    "                        Train_Loss=epoch_loss,\n",
    "                        Train_CV_R2=train_cv,\n",
    "                        LR_backbone=optimizer.optimizer1.param_groups[0]['lr'],\n",
    "                        LR_head=optimizer.optimizer2.param_groups[0]['lr'])\n",
    "    # Ensure that a parameter update is performed after the last accumulation cycle\n",
    "    if (step + 1) % CONFIG.n_accumulate != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return epoch_loss, train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.806789Z",
     "iopub.status.busy": "2025-12-24T13:44:42.806505Z",
     "iopub.status.idle": "2025-12-24T13:44:42.825919Z",
     "shell.execute_reply": "2025-12-24T13:44:42.825213Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.806769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# @torch.inference_mode()\n",
    "def valid_one_epoch(model, optimizer, valid_loader, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    with torch.no_grad():\n",
    "        for step, (images, labels) in bar:\n",
    "            batch_size = images.size(0)\n",
    "            if CONFIG.DataParallel:\n",
    "                images = images.cuda().float()\n",
    "                labels = labels.cuda().float()\n",
    "            else:\n",
    "                images = images.to(CONFIG.device, dtype=torch.float)\n",
    "                labels = labels.to(CONFIG.device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) / CONFIG.n_accumulate\n",
    "\n",
    "            y_preds.append(outputs.detach().cpu().numpy())\n",
    "            y_trues.append(labels.detach().cpu().numpy())\n",
    "            valid_cv = Calculate_Weighted_R2(np.concatenate(y_trues), np.concatenate(y_preds))\n",
    "        \n",
    "            running_loss += (loss.item() * batch_size)\n",
    "\n",
    "            dataset_size += batch_size\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "\n",
    "            bar.set_postfix(Epoch=epoch,\n",
    "                            Valid_Loss=epoch_loss,\n",
    "                            Valid_CV_R2=valid_cv,\n",
    "                            LR_backbone=optimizer.optimizer1.param_groups[0]['lr'],\n",
    "                            LR_head=optimizer.optimizer2.param_groups[0]['lr'])\n",
    "        \n",
    "\n",
    "        y_preds = np.concatenate(y_preds)\n",
    "        y_trues = np.concatenate(y_trues)\n",
    "        cv = Calculate_Weighted_R2(y_trues, y_preds) \n",
    "    \n",
    "    return epoch_loss, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.827117Z",
     "iopub.status.busy": "2025-12-24T13:44:42.826784Z",
     "iopub.status.idle": "2025-12-24T13:44:42.841714Z",
     "shell.execute_reply": "2025-12-24T13:44:42.841054Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.827090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_time_fold():\n",
    "    # Get the current time stamp\n",
    "    current_time = time.time()\n",
    "    print(\"Current timestamp:\", current_time)\n",
    "    \n",
    "    # Convert a timestamp to a local time structure\n",
    "    local_time = time.localtime(current_time)\n",
    "    \n",
    "    # Formatting local time\n",
    "    CONFIG.formatted_time = time.strftime('%Y-%m-%d_%H:%M:%S', local_time)\n",
    "    print(\"now time:\", CONFIG.formatted_time)\n",
    "    \n",
    "    CONFIG.ckpt_save_path = f\"CSIRO/output/{CONFIG.formatted_time}_{CONFIG.model_name}_output\"\n",
    "    if os.path.exists(CONFIG.ckpt_save_path) is False:\n",
    "        os.makedirs(CONFIG.ckpt_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.843443Z",
     "iopub.status.busy": "2025-12-24T13:44:42.842680Z",
     "iopub.status.idle": "2025-12-24T13:44:42.859632Z",
     "shell.execute_reply": "2025-12-24T13:44:42.858892Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.843413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_training(fold, model, optimizer, train_loader, valid_loader, num_epochs=CONFIG.epochs, now_cv=CONFIG.now_cv):\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {} x {}\\n\".format(torch.cuda.get_device_name(), torch.cuda.device_count()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_cv = now_cv\n",
    "    best_model_path = None\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "        train_epoch_loss, train_epoch_cv = train_one_epoch(model, optimizer, train_loader, epoch)\n",
    "        valid_epoch_loss, valid_epoch_cv = valid_one_epoch(model, optimizer, valid_loader, epoch)\n",
    "        print(f\"epoch: {epoch}, LOSS = {valid_epoch_loss}, CV = {valid_epoch_cv}\")\n",
    "        \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(valid_epoch_loss)\n",
    "        history['Train CV'].append(train_epoch_cv)\n",
    "        history['Valid CV'].append(valid_epoch_cv)\n",
    "        history['lr_backbone'].append(optimizer.optimizer1.param_groups[0]['lr'])\n",
    "        history['lr_head'].append(optimizer.optimizer2.param_groups[0]['lr'])\n",
    "        \n",
    "        # deep copy the model\n",
    "        if valid_epoch_cv >= best_epoch_cv:\n",
    "            print(f\"{b_}epoch: {epoch}, Validation CV Improved ({best_epoch_cv} ---> {valid_epoch_cv}))\")\n",
    "            best_epoch_cv = valid_epoch_cv\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = \"{}/{}_CV_{:.4f}_Loss{:.4f}_epoch{:.0f}.pth\".format(CONFIG.ckpt_save_path, fold, best_epoch_cv, valid_epoch_loss, epoch)\n",
    "            best_model_path = PATH\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            print(f\"Model Saved{sr_}\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best CV: {:.4f}\".format(best_epoch_cv))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history, best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.860865Z",
     "iopub.status.busy": "2025-12-24T13:44:42.860528Z",
     "iopub.status.idle": "2025-12-24T13:44:42.877217Z",
     "shell.execute_reply": "2025-12-24T13:44:42.876494Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.860836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CosineAnnealingWithWarmupLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, eta_min=0, warmup_epochs=10, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.eta_min = eta_min\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.cosine_epochs = T_max - warmup_epochs\n",
    "        super(CosineAnnealingWithWarmupLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            # Linear warmup\n",
    "            return [(base_lr * (self.last_epoch + 1) / self.warmup_epochs) for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            cosine_epoch = self.last_epoch - self.warmup_epochs\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * (1 + math.cos(math.pi * cosine_epoch / self.cosine_epochs)) / 2 for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.878479Z",
     "iopub.status.busy": "2025-12-24T13:44:42.878214Z",
     "iopub.status.idle": "2025-12-24T13:44:42.890484Z",
     "shell.execute_reply": "2025-12-24T13:44:42.889974Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.878450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "def fetch_scheduler(optimizer, T_max, min_lr):\n",
    "    if CONFIG.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=T_max, \n",
    "                                                   eta_min=min_lr)\n",
    "    elif CONFIG.scheduler == \"CosineAnnealingWithWarmupLR\":\n",
    "        scheduler = CosineAnnealingWithWarmupLR(optimizer, T_max=T_max, eta_min=min_lr, warmup_epochs=T_max//CONFIG.epochs)\n",
    "        \n",
    "    elif CONFIG.scheduler == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.891700Z",
     "iopub.status.busy": "2025-12-24T13:44:42.891390Z",
     "iopub.status.idle": "2025-12-24T13:44:42.907176Z",
     "shell.execute_reply": "2025-12-24T13:44:42.906488Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.891674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class merge_optim():\n",
    "    def __init__(self, optimizer1, optimizer2, lr_scheduler1=None, lr_scheduler2=None):\n",
    "        self.optimizer1 = optimizer1\n",
    "        self.optimizer2 = optimizer2\n",
    "        self.lr_scheduler1 = lr_scheduler1\n",
    "        self.lr_scheduler2 = lr_scheduler2\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer1.zero_grad()\n",
    "        self.optimizer2.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer1.step()\n",
    "        self.optimizer2.step()\n",
    "        if self.lr_scheduler1 is not None:\n",
    "            self.lr_scheduler1.step()\n",
    "        if self.lr_scheduler2 is not None:\n",
    "            self.lr_scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-24T13:44:42.908198Z",
     "iopub.status.busy": "2025-12-24T13:44:42.907957Z",
     "iopub.status.idle": "2025-12-24T13:44:42.922156Z",
     "shell.execute_reply": "2025-12-24T13:44:42.921376Z",
     "shell.execute_reply.started": "2025-12-24T13:44:42.908167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    if CONFIG.DataParallel:\n",
    "        optimizer_backbone = optim.AdamW(model.module.backbone.parameters(), lr=CONFIG.start_lr_backbone)\n",
    "        optimizer_head = optim.AdamW(model.module.head.parameters(), lr=CONFIG.start_lr_head)\n",
    "    else:\n",
    "        optimizer_backbone = optim.AdamW(model.backbone.parameters(), lr=CONFIG.start_lr_backbone)\n",
    "        optimizer_head = optim.AdamW(model.head.parameters(), lr=CONFIG.start_lr_head)\n",
    "    \n",
    "    scheduler_backbone = fetch_scheduler(optimizer_backbone, T_max=CONFIG.T_max, min_lr=CONFIG.min_lr_backbone)\n",
    "    scheduler_head = fetch_scheduler(optimizer_head, T_max=CONFIG.T_max, min_lr=CONFIG.min_lr_head)\n",
    "    \n",
    "    optimizer = merge_optim(optimizer_backbone, optimizer_head, scheduler_backbone, scheduler_head)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-24T13:54:23.484Z",
     "iopub.execute_input": "2025-12-24T13:44:42.923478Z",
     "iopub.status.busy": "2025-12-24T13:44:42.923160Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "oof = []\n",
    "true = []\n",
    "historys = []\n",
    "get_time_fold()\n",
    "\n",
    "for fold in range(0, CONFIG.n_folds):\n",
    "    print(f\"==================== Train on Fold {fold+1} ====================\")\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    model = CSIROModel()\n",
    "    if CONFIG.DataParallel:\n",
    "        device_ids = [0, 1]\n",
    "        model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        model = model.to(CONFIG.device)\n",
    "        \n",
    "    optimizer = get_optimizer(model)\n",
    "    \n",
    "    train_loader, valid_loader = prepare_loaders(train, fold)\n",
    "    model, history, best_model_path = run_training(fold+1, model, optimizer, \n",
    "                                                   train_loader, valid_loader, \n",
    "                                                   num_epochs=CONFIG.epochs, now_cv=CONFIG.now_cv)\n",
    "    historys.append(history)\n",
    "    \n",
    "    bar = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "    with torch.no_grad():\n",
    "        for step, (images, labels) in bar:\n",
    "            batch_size = images.size(0)\n",
    "            if CONFIG.DataParallel:\n",
    "                images = images.cuda().float()\n",
    "                labels = labels.cuda().float()\n",
    "            else:\n",
    "                images = images.to(CONFIG.device, dtype=torch.float)\n",
    "                labels = labels.to(CONFIG.device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(images)\n",
    "            \n",
    "            oof.append(outputs.detach().cpu().numpy())\n",
    "            true.append(labels.detach().cpu().numpy())\n",
    "        print()\n",
    "\n",
    "oof = np.concatenate(oof)\n",
    "true = np.concatenate(true)\n",
    "\n",
    "# 第一轮 cv\n",
    "# 0.5251 # MSELoss       + 没有正则化 + efficientnetb0\n",
    "# 0.5227 # MSELoss       + 正则化    + efficientnetb0\n",
    "# 0.5472 # WeightMSELoss + 正则化    + efficientnetb0\n",
    "# 0.8073 # WeightMSELoss + 正则化    + edgenext\n",
    "# 0.8073"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-24T13:54:23.485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "local_cv = Calculate_Weighted_R2(true, oof)\n",
    "print(\"Local CV : \", local_cv)\n",
    "\n",
    "np.save(\"CSIRO/true.npy\", true)\n",
    "np.save(f\"{CONFIG.ckpt_save_path}/oof.npy\", oof)\n",
    "\n",
    "# 0.5683843152035679 # MSELoss       + 正则化 + efficientnetb0\n",
    "# 0.5809311309267369 # WeightMSELoss + 正则化 + efficientnetb0\n",
    "# Local CV :  0.7153420406058225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-24T13:54:23.485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# fold = 0\n",
    "# history = historys[fold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-24T13:54:23.485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plt.plot( range(len(history[\"Train Loss\"])), history[\"Train Loss\"], label=\"Train Loss\")\n",
    "# plt.plot( range(len(history[\"Valid Loss\"])), history[\"Valid Loss\"], label=\"Valid Loss\")\n",
    "# plt.xlabel(\"epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-24T13:54:23.485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plt.plot( range(len(history[\"Train CV(R2)\"])), history[\"Train CV(R2)\"], label=\"Train CV(R2)\")\n",
    "# plt.plot( range(len(history[\"Valid CV(R2)\"])), history[\"Valid CV(R2)\"], label=\"Valid CV(R2)\")\n",
    "# plt.xlabel(\"epochs\")\n",
    "# plt.ylabel(\"CV(R2)\")\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-24T13:54:23.485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plt.plot( range(len(history[\"lr\"])), history[\"lr\"], label=\"lr\")\n",
    "# plt.xlabel(\"epochs\")\n",
    "# plt.ylabel(\"lr\")\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 9105797,
     "sourceId": 14269006,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
